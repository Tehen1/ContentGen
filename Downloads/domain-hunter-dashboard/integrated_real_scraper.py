#!/usr/bin/env python3
"""
Scraper Int√©gr√© avec Vraies Donn√©es
Combine la collecte de vraies donn√©es avec l'analyse Perplexity optimis√©e
"""

import sys
import os
import json
import time
import logging
import csv
import sqlite3
from typing import List, Dict, Optional
from datetime import datetime
from real_data_api import RealDataAPI

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

class IntegratedRealScraper:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.base_url = "https://api.perplexity.ai/chat/completions"
        self.headers = {
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        }
        self.data_collector = RealDataAPI()
        self.results_db = 'analyzed_real_domains.db'
        self.init_results_db()
        
    def init_results_db(self):
        """Initialise la base de donn√©es des r√©sultats d'analyse"""
        with sqlite3.connect(self.results_db) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS analyzed_domains (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    domain TEXT UNIQUE,
                    seo_score INTEGER,
                    commercial_score INTEGER,
                    competition_score INTEGER,
                    risk_score INTEGER,
                    global_score REAL,
                    recommended_price REAL,
                    projected_roi REAL,
                    recommendation TEXT,
                    analysis_type TEXT,
                    collector_score REAL,
                    estimated_value REAL,
                    analyzed_at DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            conn.commit()
    
    def collect_fresh_data(self, limit: int = 30) -> List[str]:
        """Collecte de nouvelles donn√©es r√©elles"""
        logging.info(f"üîÑ Collecte de {limit} nouveaux domaines...")
        
        # Collecte depuis les APIs
        results = self.data_collector.collect_all_sources(limit_per_source=limit//3)
        
        if results['total_collected'] > 0:
            # R√©cup√©ration des domaines collect√©s
            domains = self.data_collector.get_collected_domains(limit)
            logging.info(f"‚úÖ {len(domains)} domaines r√©els collect√©s")
            return domains
        else:
            logging.warning("‚ö†Ô∏è Aucune donn√©e collect√©e, utilisation de donn√©es existantes")
            return self.data_collector.get_collected_domains(limit)
    
    def get_domain_metadata(self, domain: str) -> Dict:
        """R√©cup√®re les m√©tadonn√©es du domaine depuis le collecteur"""
        with sqlite3.connect(self.data_collector.db_path) as conn:
            cursor = conn.execute('''
                SELECT commercial_score, keyword_relevance, estimated_value, keywords
                FROM real_domains WHERE domain = ?
            ''', (domain,))
            
            result = cursor.fetchone()
            if result:
                return {
                    'collector_commercial_score': result[0],
                    'collector_keyword_relevance': result[1],
                    'collector_estimated_value': result[2],
                    'collector_keywords': result[3]
                }
        return {}
    
    def create_enhanced_prompt(self, domain: str, metadata: Dict) -> str:
        """Cr√©e un prompt enrichi avec les m√©tadonn√©es du collecteur"""
        keywords = metadata.get('collector_keywords', '')
        estimated_value = metadata.get('collector_estimated_value', 0)
        
        prompt = f"""Analysez le domaine expir√©: {domain}

CONTEXTE ENRICHI:
- Mots-cl√©s d√©tect√©s: {keywords}
- Valeur estim√©e initiale: {estimated_value}‚Ç¨
- Secteur identifi√©: {"Tech/Business" if any(k in keywords for k in ['tech', 'business', 'ai']) else "G√©n√©raliste"}

ANALYSE EXPERTE REQUISE:

1. SEO (0-10):
   - Potentiel backlinks dans le secteur
   - Autorit√© de domaine projet√©e
   - Facilit√© de ranking

2. COMMERCIAL (0-10):
   - Mon√©tisation par affiliation
   - Potentiel publicitaire
   - Valeur pour revendeurs

3. CONCURRENCE (0-10):
   - Saturation du march√©
   - Opportunit√©s de positionnement
   - Barri√®res √† l'entr√©e

4. RISQUES (0-10):
   - Historique potentiel
   - Risques l√©gaux/marque
   - Difficult√©s techniques

5. FINANCIER:
   - Prix d'acquisition max (‚Ç¨)
   - Potentiel revente (‚Ç¨)
   - ROI 12 mois (%)

R√©pondez UNIQUEMENT en JSON:
{{"seo_score":X,"commercial_score":X,"competition_score":X,"risk_score":X,"recommended_price":X,"resale_potential":X,"projected_roi":X,"recommendation":"ACHETER/√âVITER/SURVEILLER"}}"""
        
        return prompt
    
    def analyze_real_domain(self, domain: str) -> Optional[Dict]:
        """Analyse un vrai domaine avec contexte enrichi"""
        try:
            # R√©cup√©ration des m√©tadonn√©es
            metadata = self.get_domain_metadata(domain)
            
            # Cr√©ation du prompt enrichi
            prompt = self.create_enhanced_prompt(domain, metadata)
            
            payload = {
                "model": "llama-3.1-sonar-large-128k-online",
                "messages": [
                    {"role": "system", "content": "Tu es un expert en domaines expir√©s avec acc√®s aux donn√©es de march√©. R√©ponds UNIQUEMENT en JSON valide."},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0.1,
                "max_tokens": 600
            }
            
            logging.info(f"üîç Analyse enrichie de {domain}")
            
            import requests
            response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            # Parse de la r√©ponse
            analysis = self._parse_enhanced_response(content, domain, metadata)
            
            if analysis:
                # Sauvegarde en base
                self._save_analysis(analysis)
                return analysis
            
        except Exception as e:
            logging.error(f"‚ùå Erreur analyse {domain}: {e}")
        
        return None
    
    def _parse_enhanced_response(self, content: str, domain: str, metadata: Dict) -> Optional[Dict]:
        """Parse enrichi avec m√©tadonn√©es"""
        try:
            import re
            
            # Extraction JSON
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content)
            
            if json_match:
                json_str = json_match.group(0)
                data = json.loads(json_str)
                
                # Construction du r√©sultat enrichi
                analysis = {
                    'domain': domain,
                    'seo_score': int(data.get('seo_score', 0)),
                    'commercial_score': int(data.get('commercial_score', 0)),
                    'competition_score': int(data.get('competition_score', 0)),
                    'risk_score': int(data.get('risk_score', 0)),
                    'recommended_price': float(data.get('recommended_price', 0)),
                    'resale_potential': float(data.get('resale_potential', 0)),
                    'projected_roi': float(data.get('projected_roi', 0)),
                    'recommendation': str(data.get('recommendation', 'INCONNU')),
                    'analysis_type': 'enhanced_real_data',
                    'collector_score': metadata.get('collector_commercial_score', 0),
                    'estimated_value': metadata.get('collector_estimated_value', 0),
                    'timestamp': datetime.now().isoformat()
                }
                
                # Calcul du score global enrichi
                base_scores = [
                    analysis['seo_score'], 
                    analysis['commercial_score'],
                    analysis['competition_score'], 
                    (10 - analysis['risk_score'])
                ]
                
                # Bonus du collecteur
                collector_bonus = min(metadata.get('collector_commercial_score', 0) / 10, 1.0)
                
                analysis['global_score'] = (sum(base_scores) / len(base_scores)) + collector_bonus
                
                return analysis
            
        except json.JSONDecodeError as e:
            logging.error(f"‚ùå Erreur parsing JSON {domain}: {e}")
        
        return None
    
    def _save_analysis(self, analysis: Dict):
        """Sauvegarde l'analyse en base"""
        with sqlite3.connect(self.results_db) as conn:
            conn.execute('''
                INSERT OR REPLACE INTO analyzed_domains 
                (domain, seo_score, commercial_score, competition_score, risk_score,
                 global_score, recommended_price, projected_roi, recommendation,
                 analysis_type, collector_score, estimated_value)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                analysis['domain'], analysis['seo_score'], analysis['commercial_score'],
                analysis['competition_score'], analysis['risk_score'], analysis['global_score'],
                analysis['recommended_price'], analysis['projected_roi'], analysis['recommendation'],
                analysis['analysis_type'], analysis['collector_score'], analysis['estimated_value']
            ))
            conn.commit()
    
    def batch_analyze_real_domains(self, domains: List[str]) -> List[Dict]:
        """Analyse par lots des vrais domaines"""
        logging.info(f"üöÄ Analyse par lots de {len(domains)} vrais domaines")
        
        results = []
        
        for i, domain in enumerate(domains, 1):
            logging.info(f"üìä ({i}/{len(domains)}) Analyse de {domain}")
            
            analysis = self.analyze_real_domain(domain)
            if analysis:
                results.append(analysis)
                
                # Log si c'est une opportunit√©
                if self._is_high_opportunity(analysis):
                    logging.info(f"‚≠ê OPPORTUNIT√â: {domain} - Score: {analysis['global_score']:.1f} - ROI: {analysis['projected_roi']:.1f}%")
            
            # Respect des limites API
            time.sleep(2)
        
        logging.info(f"‚úÖ Analyse termin√©e: {len(results)} domaines analys√©s")
        return results
    
    def _is_high_opportunity(self, analysis: Dict) -> bool:
        """D√©termine si c'est une opportunit√© de haute valeur"""
        return (
            analysis.get('global_score', 0) >= 7.5 and
            analysis.get('projected_roi', 0) >= 200 and
            'ACHETER' in analysis.get('recommendation', '').upper() and
            analysis.get('recommended_price', 0) <= 1000
        )
    
    def export_enhanced_results(self, filename: str = None) -> str:
        """Exporte les r√©sultats enrichis"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"real_domain_analysis_{timestamp}.csv"
        
        with sqlite3.connect(self.results_db) as conn:
            cursor = conn.execute('''
                SELECT domain, global_score, seo_score, commercial_score, projected_roi,
                       recommended_price, recommendation, collector_score, estimated_value,
                       analyzed_at
                FROM analyzed_domains
                ORDER BY global_score DESC, projected_roi DESC
            ''')
            
            results = cursor.fetchall()
        
        if results:
            fieldnames = [
                'domain', 'global_score', 'seo_score', 'commercial_score', 
                'projected_roi', 'recommended_price', 'recommendation',
                'collector_score', 'estimated_value', 'analyzed_at'
            ]
            
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                
                for result in results:
                    writer.writerow(dict(zip(fieldnames, result)))
            
            logging.info(f"üìä {len(results)} analyses export√©es vers {filename}")
            return filename
        
        return ""
    
    def get_top_opportunities(self, limit: int = 10) -> List[Dict]:
        """R√©cup√®re les meilleures opportunit√©s"""
        with sqlite3.connect(self.results_db) as conn:
            cursor = conn.execute('''
                SELECT domain, global_score, projected_roi, recommended_price, recommendation
                FROM analyzed_domains
                WHERE global_score >= 7.0 AND recommendation LIKE '%ACHETER%'
                ORDER BY global_score DESC, projected_roi DESC
                LIMIT ?
            ''', (limit,))
            
            return [dict(zip(['domain', 'global_score', 'projected_roi', 'recommended_price', 'recommendation'], row)) 
                    for row in cursor.fetchall()]

def main():
    """Fonction principale"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Scraper Int√©gr√© avec Vraies Donn√©es")
    parser.add_argument("--collect", type=int, default=20, help="Nombre de domaines √† collecter")
    parser.add_argument("--analyze", type=int, help="Nombre de domaines √† analyser")
    parser.add_argument("--fresh", action='store_true', help="Collecter de nouvelles donn√©es")
    parser.add_argument("--report", action='store_true', help="Afficher le rapport des opportunit√©s")
    
    args = parser.parse_args()
    
    # Configuration API
    PERPLEXITY_API_KEY = os.getenv("PERPLEXITY_API_KEY", "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")
    
    scraper = IntegratedRealScraper(PERPLEXITY_API_KEY)
    
    print("üåê Scraper Int√©gr√© avec Vraies Donn√©es")
    print("=" * 50)
    
    if args.fresh or args.collect:
        # Collecte de nouvelles donn√©es
        domains = scraper.collect_fresh_data(args.collect)
        print(f"üìã {len(domains)} domaines r√©els collect√©s")
    else:
        # Utilisation des donn√©es existantes
        domains = scraper.data_collector.get_collected_domains(args.analyze or 15)
        print(f"üìã {len(domains)} domaines existants charg√©s")
    
    if args.analyze or not args.report:
        # Analyse des domaines
        analyze_count = args.analyze or min(len(domains), 10)
        selected_domains = domains[:analyze_count]
        
        results = scraper.batch_analyze_real_domains(selected_domains)
        
        # Filtrage des opportunit√©s
        opportunities = [r for r in results if scraper._is_high_opportunity(r)]
        
        print(f"\nüìä R√©sultats d'analyse:")
        print(f"  ‚Ä¢ Domaines analys√©s: {len(results)}")
        print(f"  ‚Ä¢ Opportunit√©s d√©tect√©es: {len(opportunities)}")
        if results:
            print(f"  ‚Ä¢ Taux de r√©ussite: {len(opportunities)/len(results)*100:.1f}%")
        
        # Export des r√©sultats
        if results:
            csv_file = scraper.export_enhanced_results()
            print(f"üíæ R√©sultats export√©s: {csv_file}")
    
    if args.report:
        # Rapport des meilleures opportunit√©s
        opportunities = scraper.get_top_opportunities(10)
        
        if opportunities:
            print(f"\n‚≠ê Top Opportunit√©s R√©elles:")
            for i, opp in enumerate(opportunities, 1):
                print(f"  {i:2d}. {opp['domain']:25s} | Score: {opp['global_score']:5.1f} | ROI: {opp['projected_roi']:6.1f}% | Prix: {opp['recommended_price']:7.0f}‚Ç¨")
        else:
            print("\n‚ö†Ô∏è Aucune opportunit√© trouv√©e. Lancez d'abord une analyse.")
    
    print("\n‚úÖ Analyse de vraies donn√©es termin√©e !")

if __name__ == "__main__":
    main()
