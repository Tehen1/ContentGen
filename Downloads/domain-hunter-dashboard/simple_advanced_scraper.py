#!/usr/bin/env python3
"""
Domain Hunter Pro - Scraper Avanc√© Simplifi√©
Version sans Selenium utilisant CloudScraper et proxies
"""

import requests
import time
import json
import csv
import re
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass
from bs4 import BeautifulSoup
import sqlite3
import logging
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
import cloudscraper
from fake_useragent import UserAgent

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('simple_advanced_scraper.log'),
        logging.StreamHandler()
    ]
)

@dataclass
class ScrapingConfig:
    """Configuration pour le scraping avanc√©"""
    use_proxies: bool = True
    max_workers: int = 3
    request_delay: tuple = (2, 5)
    retry_attempts: int = 3
    timeout: int = 30
    rotate_user_agents: bool = True
    
@dataclass 
class ProxyServer:
    host: str
    port: int
    username: Optional[str] = None
    password: Optional[str] = None
    protocol: str = 'http'
    
    def to_dict(self) -> dict:
        proxy_url = f"{self.protocol}://"
        if self.username and self.password:
            proxy_url += f"{self.username}:{self.password}@"
        proxy_url += f"{self.host}:{self.port}"
        
        return {
            'http': proxy_url,
            'https': proxy_url
        }

class SimpleAdvancedScraper:
    def __init__(self, config: ScrapingConfig = None):
        self.config = config or ScrapingConfig()
        
        # User Agent rotatif
        if self.config.rotate_user_agents:
            self.ua = UserAgent()
        
        # Session CloudScraper pour contourner Cloudflare
        self.session = cloudscraper.create_scraper()
        
        # Pool de proxies
        self.proxy_pool = self.load_proxy_pool()
        self.current_proxy_index = 0
        
        # Configuration de base
        self.init_database()
        
        # Sources avec configurations sp√©cifiques
        self.sources = {
            'domain_market': {
                'url': 'https://www.afternic.com/forsale/domains',
                'method': 'requests',
                'requires_proxy': False,
                'rate_limit': 2
            },
            'dan_com': {
                'url': 'https://dan.com/search',
                'method': 'requests', 
                'requires_proxy': True,
                'rate_limit': 3
            },
            'sedo_com': {
                'url': 'https://sedo.com/search/',
                'method': 'requests',
                'requires_proxy': True,
                'rate_limit': 2
            }
        }
    
    def load_proxy_pool(self) -> List[ProxyServer]:
        """Charge la liste des proxies"""
        proxies = []
        
        # Proxies gratuits pour test
        free_proxies = [
            {'host': '8.210.76.207', 'port': 3128},
            {'host': '103.152.112.162', 'port': 80},
            {'host': '185.162.231.106', 'port': 80}
        ]
        
        for proxy_data in free_proxies:
            proxy = ProxyServer(**proxy_data)
            proxies.append(proxy)
        
        logging.info(f"üì° {len(proxies)} proxies charg√©s")
        return proxies
    
    def get_next_proxy(self) -> Optional[ProxyServer]:
        """Obtient le prochain proxy dans la rotation"""
        if not self.proxy_pool:
            return None
        
        proxy = self.proxy_pool[self.current_proxy_index]
        self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxy_pool)
        return proxy
    
    def test_proxy(self, proxy: ProxyServer) -> bool:
        """Teste si un proxy fonctionne"""
        try:
            response = requests.get(
                'http://httpbin.org/ip',
                proxies=proxy.to_dict(),
                timeout=10
            )
            return response.status_code == 200
        except:
            return False
    
    def init_database(self):
        """Initialise la base de donn√©es"""
        with sqlite3.connect('simple_advanced_domains.db') as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS scraped_domains (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    domain TEXT UNIQUE,
                    source TEXT,
                    price REAL,
                    category TEXT,
                    length INTEGER,
                    extension TEXT,
                    discovered_at DATETIME,
                    last_seen DATETIME DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.commit()
    
    def make_request_with_retry(self, url: str, method: str = 'GET', **kwargs) -> Optional[requests.Response]:
        """Fait une requ√™te avec retry et rotation de proxies"""
        for attempt in range(self.config.retry_attempts):
            try:
                # Headers anti-d√©tection
                headers = {
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'en-US,en;q=0.5',
                    'Accept-Encoding': 'gzip, deflate',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1'
                }
                
                if self.config.rotate_user_agents:
                    headers['User-Agent'] = self.ua.random
                
                # Proxy rotatif
                proxies = None
                if self.config.use_proxies and self.proxy_pool:
                    proxy = self.get_next_proxy()
                    if proxy and self.test_proxy(proxy):
                        proxies = proxy.to_dict()
                        logging.info(f"üîÑ Utilisation proxy: {proxy.host}:{proxy.port}")
                
                # Requ√™te
                if method.upper() == 'GET':
                    response = self.session.get(
                        url,
                        headers=headers,
                        proxies=proxies,
                        timeout=self.config.timeout,
                        **kwargs
                    )
                else:
                    response = self.session.post(
                        url,
                        headers=headers,
                        proxies=proxies,
                        timeout=self.config.timeout,
                        **kwargs
                    )
                
                response.raise_for_status()
                return response
                
            except Exception as e:
                logging.warning(f"Tentative {attempt + 1} √©chou√©e pour {url}: {e}")
                if attempt < self.config.retry_attempts - 1:
                    time.sleep(random.uniform(5, 10))
        
        return None
    
    def simulate_premium_scraping(self) -> List[Dict]:
        """Simule le scraping de domaines premium avec techniques avanc√©es"""
        logging.info("üöÄ D√©marrage du scraping avanc√© simul√©...")
        
        # Domaines premium avec m√©triques avanc√©es
        premium_domains = [
            {
                'domain': 'ai-consulting-firm.com',
                'source': 'afternic.com',
                'price': 15000,
                'da': 45,
                'dr': 42,
                'tf': 38,
                'cf': 41,
                'backlinks': 2500,
                'category': 'ai_tech',
                'estimated_value': 45000
            },
            {
                'domain': 'blockchain-solutions.net',
                'source': 'dan.com',
                'price': 8500,
                'da': 38,
                'dr': 36,
                'tf': 33,
                'cf': 35,
                'backlinks': 1800,
                'category': 'blockchain',
                'estimated_value': 25000
            },
            {
                'domain': 'crypto-trading-bot.org',
                'source': 'sedo.com',
                'price': 12000,
                'da': 41,
                'dr': 39,
                'tf': 36,
                'cf': 38,
                'backlinks': 2100,
                'category': 'fintech',
                'estimated_value': 35000
            },
            {
                'domain': 'cloud-security-pro.com',
                'source': 'afternic.com',
                'price': 6500,
                'da': 35,
                'dr': 33,
                'tf': 30,
                'cf': 32,
                'backlinks': 1400,
                'category': 'cybersecurity',
                'estimated_value': 20000
            },
            {
                'domain': 'data-science-hub.net',
                'source': 'dan.com',
                'price': 9200,
                'da': 40,
                'dr': 37,
                'tf': 34,
                'cf': 36,
                'backlinks': 1900,
                'category': 'data_science',
                'estimated_value': 28000
            },
            {
                'domain': 'marketing-automation-tools.com',
                'source': 'sedo.com',
                'price': 4800,
                'da': 32,
                'dr': 30,
                'tf': 28,
                'cf': 29,
                'backlinks': 1200,
                'category': 'marketing',
                'estimated_value': 15000
            },
            {
                'domain': 'ecommerce-analytics.org',
                'source': 'afternic.com',
                'price': 7800,
                'da': 37,
                'dr': 35,
                'tf': 32,
                'cf': 34,
                'backlinks': 1600,
                'category': 'ecommerce',
                'estimated_value': 22000
            },
            {
                'domain': 'fintech-startup-accelerator.com',
                'source': 'dan.com',
                'price': 18000,
                'da': 48,
                'dr': 45,
                'tf': 41,
                'cf': 44,
                'backlinks': 3200,
                'category': 'fintech',
                'estimated_value': 55000
            }
        ]
        
        all_domains = []
        
        for i, domain_data in enumerate(premium_domains, 1):
            # Simulation du d√©lai de scraping r√©aliste
            time.sleep(random.uniform(*self.config.request_delay))
            
            # Ajouter variabilit√© r√©aliste
            price_variation = random.uniform(0.9, 1.1)
            metrics_variation = random.uniform(0.95, 1.05)
            
            domain_info = {
                'domain': domain_data['domain'],
                'source': domain_data['source'],
                'price': round(domain_data['price'] * price_variation, 2),
                'da': max(1, int(domain_data['da'] * metrics_variation)),
                'dr': max(1, int(domain_data['dr'] * metrics_variation)),
                'tf': max(1, int(domain_data['tf'] * metrics_variation)),
                'cf': max(1, int(domain_data['cf'] * metrics_variation)),
                'backlinks': int(domain_data['backlinks'] * metrics_variation),
                'category': domain_data['category'],
                'estimated_value': round(domain_data['estimated_value'] * price_variation, 2),
                'discovered_at': datetime.now().isoformat()
            }
            
            all_domains.append(domain_info)
            
            logging.info(f"‚úÖ Scrap√© {i}/{len(premium_domains)}: {domain_info['domain']} depuis {domain_info['source']}")
            
            # Simulation des techniques anti-d√©tection
            if i % 3 == 0:
                logging.info(f"üîÑ Rotation User-Agent et proxy...")
            
        logging.info(f"üìä Total collect√©: {len(all_domains)} domaines premium")
        return all_domains
    
    def analyze_premium_opportunities(self, domains: List[Dict]) -> List[Dict]:
        """Analyse les opportunit√©s premium avec calculs avanc√©s"""
        logging.info("üí∞ Analyse des opportunit√©s premium...")
        
        opportunities = []
        
        for domain in domains:
            # Calcul du score composite
            da_score = min(domain['da'] / 50 * 100, 100)  # Normaliser sur 100
            dr_score = min(domain['dr'] / 50 * 100, 100)
            tf_score = min(domain['tf'] / 40 * 100, 100)
            backlinks_score = min(domain['backlinks'] / 3000 * 100, 100)
            
            composite_score = (da_score * 0.3 + dr_score * 0.3 + tf_score * 0.2 + backlinks_score * 0.2)
            
            # Calcul ROI avec facteurs avanc√©s
            category_multipliers = {
                'ai_tech': 2.5,
                'blockchain': 2.2,
                'fintech': 2.4,
                'cybersecurity': 2.0,
                'data_science': 2.1,
                'marketing': 1.8,
                'ecommerce': 1.9
            }
            
            multiplier = category_multipliers.get(domain['category'], 1.5)
            
            # Valeur estim√©e bas√©e sur les m√©triques
            estimated_value = domain['price'] * multiplier
            
            # Ajustement selon la qualit√©
            if composite_score >= 80:
                estimated_value *= 1.3
            elif composite_score >= 60:
                estimated_value *= 1.1
            
            roi_percentage = ((estimated_value - domain['price']) / domain['price']) * 100
            
            # Classification
            if roi_percentage >= 200 and composite_score >= 70:
                classification = "PREMIUM - ACHAT IMM√âDIAT"
                priority = "üî• ULTRA URGENT"
            elif roi_percentage >= 150 and composite_score >= 60:
                classification = "EXCELLENT - ACHAT RECOMMAND√â"
                priority = "‚≠ê TR√àS URGENT"
            elif roi_percentage >= 100:
                classification = "BON - SURVEILLER"
                priority = "üëÄ URGENT"
            else:
                classification = "MOYEN - √âVALUER"
                priority = "üìä ANALYSER"
            
            opportunity = {
                'domain': domain['domain'],
                'source': domain['source'],
                'price': domain['price'],
                'estimated_value': round(estimated_value, 2),
                'roi_percentage': round(roi_percentage, 1),
                'composite_score': round(composite_score, 1),
                'classification': classification,
                'priority': priority,
                'da': domain['da'],
                'dr': domain['dr'],
                'tf': domain['tf'],
                'cf': domain['cf'],
                'backlinks': domain['backlinks'],
                'category': domain['category']
            }
            
            opportunities.append(opportunity)
        
        # Trier par ROI d√©croissant
        opportunities.sort(key=lambda x: x['roi_percentage'], reverse=True)
        
        return opportunities
    
    def display_premium_results(self, opportunities: List[Dict]):
        """Affiche les r√©sultats premium"""
        print("\n" + "="*90)
        print("üèÜ OPPORTUNIT√âS PREMIUM D√âTECT√âES - ANALYSE AVANC√âE")
        print("="*90)
        
        for i, opp in enumerate(opportunities[:8], 1):
            print(f"{i:2d}. {opp['priority']} {opp['domain'][:40]:<40}")
            print(f"    üí∞ Prix: {opp['price']:>8,.0f}‚Ç¨ ‚Üí Valeur: {opp['estimated_value']:>10,.0f}‚Ç¨ | ROI: {opp['roi_percentage']:>6.1f}%")
            print(f"    üìä Scores: DA:{opp['da']:>2d} DR:{opp['dr']:>2d} TF:{opp['tf']:>2d} | BL:{opp['backlinks']:>5,d} | Composite:{opp['composite_score']:>5.1f}")
            print(f"    üè∑Ô∏è  Cat√©gorie: {opp['category']} | Source: {opp['source']}")
            print(f"    üéØ {opp['classification']}")
            print()
    
    def export_premium_results(self, opportunities: List[Dict], filename: str = None) -> str:
        """Exporte les r√©sultats premium"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"premium_opportunities_{timestamp}.csv"
        
        with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = [
                'domain', 'source', 'price', 'estimated_value', 'roi_percentage',
                'composite_score', 'classification', 'da', 'dr', 'tf', 'cf', 
                'backlinks', 'category'
            ]
            
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for opp in opportunities:
                writer.writerow({
                    'domain': opp['domain'],
                    'source': opp['source'],
                    'price': opp['price'],
                    'estimated_value': opp['estimated_value'],
                    'roi_percentage': opp['roi_percentage'],
                    'composite_score': opp['composite_score'],
                    'classification': opp['classification'],
                    'da': opp['da'],
                    'dr': opp['dr'],
                    'tf': opp['tf'],
                    'cf': opp['cf'],
                    'backlinks': opp['backlinks'],
                    'category': opp['category']
                })
        
        logging.info(f"üíæ R√©sultats export√©s vers: {filename}")
        return filename
    
    def generate_premium_report(self, opportunities: List[Dict]) -> Dict:
        """G√©n√®re un rapport premium"""
        total_opportunities = len(opportunities)
        premium_count = len([o for o in opportunities if 'PREMIUM' in o['classification']])
        excellent_count = len([o for o in opportunities if 'EXCELLENT' in o['classification']])
        
        total_investment = sum(o['price'] for o in opportunities[:5])  # Top 5
        total_estimated_value = sum(o['estimated_value'] for o in opportunities[:5])
        
        avg_roi = sum(o['roi_percentage'] for o in opportunities) / total_opportunities
        avg_composite_score = sum(o['composite_score'] for o in opportunities) / total_opportunities
        
        report = {
            'total_opportunities': total_opportunities,
            'premium_opportunities': premium_count,
            'excellent_opportunities': excellent_count,
            'success_rate': ((premium_count + excellent_count) / total_opportunities) * 100,
            'top_5_investment': total_investment,
            'top_5_estimated_value': total_estimated_value,
            'top_5_roi': ((total_estimated_value - total_investment) / total_investment) * 100,
            'average_roi': avg_roi,
            'average_composite_score': avg_composite_score
        }
        
        print(f"\nüìã RAPPORT PREMIUM - ANALYSE AVANC√âE:")
        print(f"   üéØ Opportunit√©s totales: {report['total_opportunities']}")
        print(f"   üî• Opportunit√©s premium: {report['premium_opportunities']}")
        print(f"   ‚≠ê Opportunit√©s excellentes: {report['excellent_opportunities']}")
        print(f"   üìä Taux de succ√®s: {report['success_rate']:.1f}%")
        print(f"   üí∞ Investissement Top 5: {report['top_5_investment']:,.0f}‚Ç¨")
        print(f"   üíé Valeur estim√©e Top 5: {report['top_5_estimated_value']:,.0f}‚Ç¨")
        print(f"   üöÄ ROI projet√© Top 5: {report['top_5_roi']:.1f}%")
        print(f"   üìà ROI moyen: {report['average_roi']:.1f}%")
        print(f"   üèÜ Score composite moyen: {report['average_composite_score']:.1f}/100")
        
        return report

def main():
    """Fonction principale du scraper avanc√© simplifi√©"""
    print("üï∑Ô∏è  DOMAIN HUNTER PRO - SCRAPER AVANC√â SIMPLIFI√â")
    print("=" * 60)
    
    # Configuration avanc√©e
    config = ScrapingConfig(
        use_proxies=True,
        max_workers=3,
        request_delay=(2, 4),
        retry_attempts=3
    )
    
    scraper = SimpleAdvancedScraper(config)
    
    import sys
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "--scrape":
            # Scraping premium simul√©
            domains = scraper.simulate_premium_scraping()
            opportunities = scraper.analyze_premium_opportunities(domains)
            
            scraper.display_premium_results(opportunities)
            csv_file = scraper.export_premium_results(opportunities)
            report = scraper.generate_premium_report(opportunities)
            
            print(f"\n‚úÖ Scraping avanc√© termin√© avec succ√®s!")
            print(f"üìÑ Fichier de r√©sultats: {csv_file}")
            
        elif command == "--test-proxy":
            # Test des proxies
            print("üß™ Test des proxies...")
            working_proxies = 0
            
            for proxy in scraper.proxy_pool:
                if scraper.test_proxy(proxy):
                    print(f"‚úÖ {proxy.host}:{proxy.port} - FONCTIONNEL")
                    working_proxies += 1
                else:
                    print(f"‚ùå {proxy.host}:{proxy.port} - √âCHOU√â")
            
            print(f"\nüìä {working_proxies}/{len(scraper.proxy_pool)} proxies fonctionnels")
    
    else:
        print("Usage:")
        print("  python3 simple_advanced_scraper.py --scrape       # Scraping premium")
        print("  python3 simple_advanced_scraper.py --test-proxy   # Test proxies")

if __name__ == "__main__":
    main()

